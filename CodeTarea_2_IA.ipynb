{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Código Tarea 2 - Carlos Andre Nuñez Duran**"
      ],
      "metadata": {
        "id": "foUbYMdPtnJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Universidad de O'Higgins\n",
        "\n",
        "## Escuela de Ingeniería\n",
        "## COM4402: Introducción a Inteligencia Artificial\n",
        "\n",
        "### **Tarea 2: Clasificación de Dígitos Manuscritos con Redes Neuronales**\n",
        "\n",
        "### Estudiante: Ingrese su nombre y apellido\n",
        "\n",
        "El objetivo de esta tarea es utilizar redes neuronales en un problema de clasificación de dígitos. Se utilizará el conjunto de datos Optical Recognition of Handwritten Digits Data Set. Este conjunto tiene 64 características, con 10 clases y 5620 muestras en total. La base de datos estará disponible en U-Campus.\n",
        "\n",
        "Las redes a ser entrenadas tienen la siguiente estructura: capa de entrada de dimensionalidad 64 (correspondiente a los datos de entrada), capas ocultas (una o dos) y capa de salida con 10 neuronas y función de activación softmax. La función de loss (pérdida) es entropía cruzada. El optimizador que se\n",
        "debe usar es Adam. La función softmax está implícita al usar la función de pérdida CrossEntropyLoss de PyTorch (**no se debe agregar softmax a la salida de la red**).\n",
        "\n",
        "Se usará PyTorch para entrenar y validar la red neuronal que implementa el clasificador de dígitos. Se analizará los efectos de cambiar el tamaño de la red (número de capas ocultas y de neuronas en estas\n",
        "capas) y la función de activación.\n",
        "\n",
        "El siguiente código base debe ser usado para realizar las actividades pedidas."
      ],
      "metadata": {
        "id": "Qmo_gUaKrBgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observación: Antes de ejecutar su código, active el uso de GPU en Google Colab para acelerar el proceso de entrenamiento.\n",
        "\n",
        "### Para esto: vaya a \"Entorno de Ejecución\" en el menú superior, haga click en \"Cambiar tipo de entorno de ejecución\", y seleccionar/verificar \"GPU\" en \"Acelerador de Hardware\""
      ],
      "metadata": {
        "id": "1TYWhoCvsn-8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCZ7oF6sR-Po"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi3AQdmgVwtd"
      },
      "source": [
        "## Subir datasets de dígitos (train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive                                                  # conectar con google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Felipe1401/Mineria/main/dataset_digits/1_digits_train.txt  # importo 1_digits_train.txt\n",
        "!wget https://raw.githubusercontent.com/Felipe1401/Mineria/main/dataset_digits/1_digits_test.txt   # importo 1_digits_test.txt\n"
      ],
      "metadata": {
        "id": "_0SaCJWc7_1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKJ_qwMmV9As"
      },
      "source": [
        "## Leer dataset de dígitos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PY8i4KEZnnN"
      },
      "source": [
        "column_names = [\"feat\" + str(i) for i in range(64)]\n",
        "column_names.append(\"class\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ps17buyVt-3"
      },
      "source": [
        "df_train_val = pd.read_csv('1_digits_train.txt', names = column_names)\n",
        "df_train_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('1_digits_test.txt', names = column_names)\n",
        "df_test"
      ],
      "metadata": {
        "id": "sbZ5e_EUw3Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = train_test_split(df_train_val, test_size = 0.3, random_state = 10)"
      ],
      "metadata": {
        "id": "ix_HpARUw_ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler().fit(df_train.iloc[:,0:64])\n",
        "df_train.iloc[:,0:64] = scaler.transform(df_train.iloc[:,0:64])\n",
        "df_val.iloc[:,0:64] = scaler.transform(df_val.iloc[:,0:64])\n",
        "df_test.iloc[:,0:64] = scaler.transform(df_test.iloc[:,0:64])"
      ],
      "metadata": {
        "id": "Za_qplJvw8K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "ChtKZkfyxKOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-3-bU2IxSA4"
      },
      "source": [
        "## Crear modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVqQYb9wcb27"
      },
      "source": [
        "model = nn.Sequential(\n",
        "          nn.Linear(64, 10),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(10,10)\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLBTy_3JdKhs"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdHaG51qxcuI"
      },
      "source": [
        "## Crear datasets y dataloaders para pytorch (train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htP8BlaJe41j"
      },
      "source": [
        "# Crear datasets\n",
        "feats_train = df_train.to_numpy()[:,0:64].astype(np.float32)\n",
        "labels_train = df_train.to_numpy()[:,64].astype(int)\n",
        "dataset_train = [ {\"features\":feats_train[i,:], \"labels\":labels_train[i]} for i in range(feats_train.shape[0]) ]\n",
        "\n",
        "feats_val = df_val.to_numpy()[:,0:64].astype(np.float32)\n",
        "labels_val = df_val.to_numpy()[:,64].astype(int)\n",
        "dataset_val = [ {\"features\":feats_val[i,:], \"labels\":labels_val[i]} for i in range(feats_val.shape[0]) ]\n",
        "\n",
        "feats_test = df_test.to_numpy()[:,0:64].astype(np.float32)\n",
        "labels_test = df_test.to_numpy()[:,64].astype(int)\n",
        "dataset_test = [ {\"features\":feats_test[i,:], \"labels\":labels_test[i]} for i in range(feats_test.shape[0]) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBbi9klod9IE"
      },
      "source": [
        "# Crear dataloaders\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True, num_workers=0)\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=128, shuffle=True, num_workers=0)\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=128, shuffle=True, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-nVLEmvxlAQ"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeWnOtyOdijh"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "# loop over the dataset multiple times\n",
        "for epoch in range(250):\n",
        "  model.train()\n",
        "  # Train on the current epoch\n",
        "  for i, data in enumerate(dataloader_train, 0):\n",
        "    # Process the current batch\n",
        "    inputs = data[\"features\"].to(device)\n",
        "    labels = data[\"labels\"].to(device)\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward() # backpropagation\n",
        "    optimizer.step()\n",
        "\n",
        "    # Por completar: calcule la pérdida de validación y acurracy en el batch actual\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # Por completar: calcule la pérdida de validación y acurracy en la época actual\n",
        "    pass\n",
        "\n",
        "  # Por hacer: imprima la pérdida de entrenamiento/validación y acurracy en la época actual\n",
        "  print('epoch %d' % (epoch))\n",
        "\n",
        "end = time.time()\n",
        "print('Finished Training, total time %f seconds' % (end - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARTE 2"
      ],
      "metadata": {
        "id": "kUMxMrhWrlPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader_train, dataloader_val, epochs=1000, patience=20):\n",
        "    \"\"\"\n",
        "    Función para entrenar el modelo con early stopping basado en el loss de validación.\n",
        "\n",
        "    Parámetros:\n",
        "    - model: Modelo de PyTorch que se entrenará.\n",
        "    - dataloader_train: Dataloader para el conjunto de entrenamiento.\n",
        "    - dataloader_val: Dataloader para el conjunto de validación.\n",
        "    - epochs: Número máximo de épocas para entrenar el modelo.\n",
        "    - patience: Número de épocas a esperar sin mejora en el loss de validación antes de detener el entrenamiento.\n",
        "\n",
        "    Retorna:\n",
        "    - train_loss_history: Lista con el historial de loss de entrenamiento a lo largo de las épocas.\n",
        "    - val_loss_history: Lista con el historial de loss de validación a lo largo de las épocas.\n",
        "    \"\"\"\n",
        "\n",
        "    # Establecemos el dispositivo de cálculo (GPU si está disponible, de lo contrario CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Definimos la función de pérdida y el optimizador\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Inicializamos el mejor loss de validación con un valor infinitamente grande\n",
        "    best_val_loss = float('inf')\n",
        "    # Listas para almacenar los historiales de loss de entrenamiento y validación\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    # Contador para la paciencia del early stopping\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Bucle principal de entrenamiento\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Pone el modelo en modo de entrenamiento\n",
        "        train_loss = 0.0\n",
        "        for data in dataloader_train:\n",
        "            # Cargamos los datos y etiquetas al dispositivo adecuado\n",
        "            inputs, labels = data[\"features\"].to(device), data[\"labels\"].to(device)\n",
        "            # Reiniciamos los gradientes del optimizador\n",
        "            optimizer.zero_grad()\n",
        "            # Realizamos una pasada hacia adelante\n",
        "            outputs = model(inputs)\n",
        "            # Calculamos el loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            # Actualizamos los pesos del modelo\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "        # Calculamos el loss promedio de entrenamiento para esta época\n",
        "        train_loss = train_loss / len(dataloader_train.dataset)\n",
        "        train_loss_history.append(train_loss)\n",
        "\n",
        "        # Evaluamos el modelo en el conjunto de validación\n",
        "        model.eval()  # Pone el modelo en modo de evaluación\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for data in dataloader_val:\n",
        "                inputs, labels = data[\"features\"].to(device), data[\"labels\"].to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "        # Calculamos el loss promedio de validación para esta época\n",
        "        val_loss = val_loss / len(dataloader_val.dataset)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        # Comprobamos si el loss de validación ha mejorado\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0  # Reiniciamos el contador de paciencia\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            # Si no ha habido mejora en el número de épocas definido por \"patience\", detenemos el entrenamiento\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    return train_loss_history, val_loss_history\n"
      ],
      "metadata": {
        "id": "coqX_dx9rU-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_val_loss(train_loss_history, val_loss_history):\n",
        "    \"\"\"\n",
        "    Función para graficar el loss de entrenamiento y validación a lo largo de las épocas.\n",
        "\n",
        "    Parámetros:\n",
        "    - train_loss_history: Lista con el historial de loss de entrenamiento.\n",
        "    - val_loss_history: Lista con el historial de loss de validación.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.plot(train_loss_history, label=\"Train Loss\")\n",
        "    plt.plot(val_loss_history, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Train vs. Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4i4ZCSujraUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la función compute_confusion_matrix\n",
        "def compute_confusion_matrix(model, dataloader):\n",
        "    \"\"\"\n",
        "    Calcula y devuelve la matriz de confusión para el modelo y dataloader proporcionados.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data[\"features\"].to(device), data[\"labels\"].to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Normalizar la matriz de confusión\n",
        "    conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    return conf_matrix\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix):\n",
        "    \"\"\"\n",
        "    Función para graficar la matriz de confusión.\n",
        "\n",
        "    Parámetros:\n",
        "    - conf_matrix: Matriz de confusión a graficar.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # Usamos un formato personalizado para mostrar sólo dos decimales\n",
        "    sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt=\".2f\", cbar=False)\n",
        "\n",
        "    ax.set_xlabel('Predicted labels')\n",
        "    ax.set_ylabel('True labels')\n",
        "    ax.set_title('Confusion Matrix')\n",
        "    ax.xaxis.set_ticklabels(range(10))\n",
        "    ax.yaxis.set_ticklabels(range(10))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "eaoXuQm1rhVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    \"\"\"\n",
        "    Evalúa el modelo en un conjunto de datos y calcula la matriz de confusión y el accuracy.\n",
        "\n",
        "    Parámetros:\n",
        "    - model: Modelo de PyTorch a evaluar.\n",
        "    - dataloader: Dataloader del conjunto de datos en el que se evaluará el modelo.\n",
        "\n",
        "    Retorna:\n",
        "    - conf_matrix: Matriz de confusión.\n",
        "    - acc: Accuracy del modelo en el conjunto de datos.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    conf_matrix = np.zeros((10, 10))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data[\"features\"].to(device), data[\"labels\"].to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "                conf_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "    acc = correct / total\n",
        "    return conf_matrix, acc\n",
        "\n"
      ],
      "metadata": {
        "id": "VMWYVnzXrgFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rhroFUshvqM"
      },
      "source": [
        "# Definición de Modelos y Entrenamiento\n",
        "\n",
        "# (a) 10 neuronas en la capa oculta, usando función de activación ReLU y 1000 épocas como máximo.\n",
        "model_a = nn.Sequential(\n",
        "    nn.Linear(64, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 10)\n",
        ")\n",
        "train_loss_history_a, val_loss_history_a = train_model(model_a, dataloader_train, dataloader_val, epochs=1000)\n",
        "plot_train_val_loss(train_loss_history_a, val_loss_history_a)\n",
        "\n",
        "# (b) 40 neuronas en la capa oculta y función de activación ReLU, y 1000 épocas como máximo.\n",
        "model_b = nn.Sequential(\n",
        "    nn.Linear(64, 40),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(40, 10)\n",
        ")\n",
        "train_loss_history_b, val_loss_history_b = train_model(model_b, dataloader_train, dataloader_val, epochs=1000)\n",
        "plot_train_val_loss(train_loss_history_b, val_loss_history_b)\n",
        "\n",
        "# (c) 10 neuronas en la capa oculta y función de activación Tanh, y 1000 épocas como máximo.\n",
        "model_c = nn.Sequential(\n",
        "    nn.Linear(64, 10),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(10, 10)\n",
        ")\n",
        "train_loss_history_c, val_loss_history_c = train_model(model_c, dataloader_train, dataloader_val, epochs=1000)\n",
        "plot_train_val_loss(train_loss_history_c, val_loss_history_c)\n",
        "\n",
        "# (d) 40 neuronas en la capa oculta y función de activación Tanh, y 1000 épocas como máximo.\n",
        "model_d = nn.Sequential(\n",
        "    nn.Linear(64, 40),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(40, 10)\n",
        ")\n",
        "train_loss_history_d, val_loss_history_d = train_model(model_d, dataloader_train, dataloader_val, epochs=1000)\n",
        "plot_train_val_loss(train_loss_history_d, val_loss_history_d)\n",
        "\n",
        "# (e) 2 capas ocultas con 10 y 10 neuronas cada una y función de activación ReLU, y 1000 épocas como máximo.\n",
        "model_e = nn.Sequential(\n",
        "    nn.Linear(64, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 10)\n",
        ")\n",
        "train_loss_history_e, val_loss_history_e = train_model(model_e, dataloader_train, dataloader_val, epochs=1000)\n",
        "plot_train_val_loss(train_loss_history_e, val_loss_history_e)\n",
        "\n",
        "# (f) 2 capas ocultas con 40 y 40 neuronas cada una y función de activación ReLU, y 1000 épocas como máximo.\n",
        "model_f = nn.Sequential(\n",
        "    nn.Linear(64, 40),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(40, 40),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(40, 10)\n",
        ")\n",
        "train_loss_history_f, val_loss_history_f = train_model(model_f, dataloader_train, dataloader_val, epochs=1000)\n",
        "plot_train_val_loss(train_loss_history_f, val_loss_history_f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_confusion_matrix(conf_matrix, dataset_type):\n",
        "    \"\"\"\n",
        "    Función para graficar la matriz de confusión.\n",
        "\n",
        "    Parámetros:\n",
        "    - conf_matrix: Matriz de confusión a graficar.\n",
        "    - dataset_type: 'test' o 'val' para especificar el tipo de conjunto en el título.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt=\".2f\", cbar=False)\n",
        "\n",
        "    ax.set_xlabel('Predicted labels')\n",
        "    ax.set_ylabel('True labels')\n",
        "    ax.set_title(f'Confusion Matrix for {dataset_type} set')\n",
        "    ax.xaxis.set_ticklabels(range(10))\n",
        "    ax.yaxis.set_ticklabels(range(10))\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_and_plot_case(model, dataloader_test, dataloader_val, case_label):\n",
        "    # Test\n",
        "    plot_train_val_loss(eval(f'train_loss_history_{case_label}'), eval(f'val_loss_history_{case_label}'))\n",
        "    conf_matrix_test = compute_confusion_matrix(model, dataloader_test)\n",
        "    accuracy_test = (np.diag(conf_matrix_test).sum() / conf_matrix_test.sum()) * 100\n",
        "    print(f\"Caso ({case_label}) - Accuracy en Test: {accuracy_test:.2f}%\")\n",
        "    plot_confusion_matrix(conf_matrix_test, \"test\")\n",
        "\n",
        "    # Validación\n",
        "    conf_matrix_val = compute_confusion_matrix(model, dataloader_val)\n",
        "    accuracy_val = (np.diag(conf_matrix_val).sum() / conf_matrix_val.sum()) * 100\n",
        "    print(f\"Caso ({case_label}) - Accuracy en Validación: {accuracy_val:.2f}%\")\n",
        "    plot_confusion_matrix(conf_matrix_val, \"validation\")\n",
        "\n",
        "#Llamo la función para cada caso:\n",
        "\n",
        "evaluate_and_plot_case(model_a, dataloader_test, dataloader_val, 'a')\n",
        "evaluate_and_plot_case(model_b, dataloader_test, dataloader_val, 'b')\n",
        "evaluate_and_plot_case(model_c, dataloader_test, dataloader_val, 'c')\n",
        "evaluate_and_plot_case(model_d, dataloader_test, dataloader_val, 'd')\n",
        "evaluate_and_plot_case(model_e, dataloader_test, dataloader_val, 'e')\n",
        "evaluate_and_plot_case(model_f, dataloader_test, dataloader_val, 'f')\n"
      ],
      "metadata": {
        "id": "ccdCCGBr_Xt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluamos el accuracy de validación para cada modelo para buscar el mejor\n",
        "\n",
        "def validation_accuracy(model, dataloader_val):\n",
        "    \"\"\"\n",
        "    Función para calcular el accuracy de validación de un modelo.\n",
        "    \"\"\"\n",
        "    conf_matrix, acc = evaluate_model(model, dataloader_val)\n",
        "    return acc\n",
        "\n",
        "# Calculamos el accuracy de validación para cada modelo\n",
        "acc_a = validation_accuracy(model_a, dataloader_val)\n",
        "acc_b = validation_accuracy(model_b, dataloader_val)\n",
        "acc_c = validation_accuracy(model_c, dataloader_val)\n",
        "acc_d = validation_accuracy(model_d, dataloader_val)\n",
        "acc_e = validation_accuracy(model_e, dataloader_val)\n",
        "acc_f = validation_accuracy(model_f, dataloader_val)\n",
        "\n",
        "# Mostramos los accuracies\n",
        "print(f\"Accuracy de validación para modelo (a): {acc_a * 100:.2f}%\")\n",
        "print(f\"Accuracy de validación para modelo (b): {acc_b * 100:.2f}%\")\n",
        "print(f\"Accuracy de validación para modelo (c): {acc_c * 100:.2f}%\")\n",
        "print(f\"Accuracy de validación para modelo (d): {acc_d * 100:.2f}%\")\n",
        "print(f\"Accuracy de validación para modelo (e): {acc_e * 100:.2f}%\")\n",
        "print(f\"Accuracy de validación para modelo (f): {acc_f * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "hBdUyj6csqFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificamos y mostramos el modelo con el mayor accuracy de validación\n",
        "models = [model_a, model_b, model_c, model_d, model_e, model_f]\n",
        "accuracies = [acc_a, acc_b, acc_c, acc_d, acc_e, acc_f]\n",
        "best_index = accuracies.index(max(accuracies))\n",
        "best_model = models[best_index]\n",
        "\n",
        "print(f\"El mejor modelo es el modelo ({chr(97 + best_index)}) con un accuracy de validación de {accuracies[best_index] * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "DEQ5iznnsuSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Calcular la matriz de confusión normalizada para el conjunto de prueba.\n",
        "best_conf_matrix_test = compute_confusion_matrix(best_model, dataloader_test)\n",
        "\n",
        "# 3. Calcular el accuracy normalizado usando el conjunto de prueba.\n",
        "best_accuracy_test = (np.diag(best_conf_matrix_test).sum() / best_conf_matrix_test.sum()) * 100\n",
        "\n",
        "# Mostramos la matriz de confusión y el accuracy para el mejor modelo\n",
        "print(f\"Mejor modelo ({chr(97 + best_index)}) - Accuracy en conjunto de prueba: {best_accuracy_test:.2f}%\")\n",
        "plot_confusion_matrix(best_conf_matrix_test, \"test\")\n"
      ],
      "metadata": {
        "id": "voDHpUlYBTV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **El siguiente código es solo para hacer un analisis de los resultados**"
      ],
      "metadata": {
        "id": "vaqBxn0Lgv4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_nn(input_size, output_size, neurons, activation):\n",
        "    \"\"\"\n",
        "    Crea una red neuronal basada en la configuración especificada.\n",
        "\n",
        "    Parámetros:\n",
        "    - input_size: Número de entradas de la red.\n",
        "    - output_size: Número de salidas de la red.\n",
        "    - neurons: Lista con el número de neuronas en cada capa oculta.\n",
        "    - activation: Función de activación a usar.\n",
        "\n",
        "    Retorna:\n",
        "    - model: Modelo de red neuronal.\n",
        "    \"\"\"\n",
        "\n",
        "    layers = []\n",
        "    input_dim = input_size\n",
        "\n",
        "    # Agregamos las capas ocultas\n",
        "    for n in neurons:\n",
        "        layers.append(nn.Linear(input_dim, n))\n",
        "        layers.append(activation)\n",
        "        input_dim = n\n",
        "\n",
        "    # Capa de salida\n",
        "    layers.append(nn.Linear(input_dim, output_size))\n",
        "\n",
        "    model = nn.Sequential(*layers)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Definimos una lista de configuraciones basada en los requerimientos\n",
        "configurations = [\n",
        "    {\"neurons\": [10], \"activation\": nn.ReLU()},\n",
        "    {\"neurons\": [40], \"activation\": nn.ReLU()},\n",
        "    {\"neurons\": [10], \"activation\": nn.Tanh()},\n",
        "    {\"neurons\": [40], \"activation\": nn.Tanh()},\n",
        "    {\"neurons\": [10, 10], \"activation\": nn.ReLU()},\n",
        "    {\"neurons\": [40, 40], \"activation\": nn.ReLU()}\n",
        "]\n",
        "\n",
        "# Diccionario para almacenar los resultados\n",
        "results = {}\n",
        "\n",
        "# Entrenamos y evaluamos cada configuración\n",
        "for i, config in enumerate(configurations):\n",
        "    model = create_nn(input_size=64, output_size=10, neurons=config[\"neurons\"], activation=config[\"activation\"])\n",
        "    start_time = time.time()\n",
        "    train_loss, val_loss = train_model(model, dataloader_train, dataloader_val)\n",
        "    end_time = time.time()\n",
        "\n",
        "    conf_matrix_val, acc_val = evaluate_model(model, dataloader_val)\n",
        "    conf_matrix_test, acc_test = evaluate_model(model, dataloader_test)\n",
        "\n",
        "    results[i] = {\n",
        "        \"Configuration\": config,\n",
        "        \"Training Time\": end_time - start_time,\n",
        "        \"Train Loss History\": train_loss,\n",
        "        \"Validation Loss History\": val_loss,\n",
        "        \"Validation Confusion Matrix\": conf_matrix_val,\n",
        "        \"Validation Accuracy\": acc_val,\n",
        "        \"Test Confusion Matrix\": conf_matrix_test,\n",
        "        \"Test Accuracy\": acc_test\n",
        "    }\n",
        "\n",
        "# Imprimimos los resultados de manera estructurada\n",
        "for i, result in results.items():\n",
        "    print(f\"Configuration {i + 1}:\")\n",
        "    print(\"Neurons:\", result[\"Configuration\"][\"neurons\"])\n",
        "    print(\"Activation:\", type(result[\"Configuration\"][\"activation\"]).__name__)\n",
        "    print(\"Training Time:\", result[\"Training Time\"])\n",
        "    print(\"Validation Accuracy:\", result[\"Validation Accuracy\"])\n",
        "    print(\"Test Accuracy:\", result[\"Test Accuracy\"])\n",
        "    print(\"Last 5 Train Losses:\", result[\"Train Loss History\"][-5:])\n",
        "    print(\"Last 5 Validation Losses:\", result[\"Validation Loss History\"][-5:])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "KK7WS3nALDXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **El siguiente codigo es solo para graficar las matrices de forma comoda para el informe**"
      ],
      "metadata": {
        "id": "KuHlBTPUtDEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_two_confusion_matrices(cm1, cm2, labels, filename):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    sns.heatmap(cm1, annot=True, ax=axs[0], cmap='Blues', fmt=\".2f\", cbar=False)\n",
        "    axs[0].set_xlabel('Predicted labels')\n",
        "    axs[0].set_ylabel('True labels')\n",
        "    axs[0].set_title('Validation Confusion Matrix')\n",
        "    axs[0].xaxis.set_ticklabels(labels)\n",
        "    axs[0].yaxis.set_ticklabels(labels)\n",
        "\n",
        "    sns.heatmap(cm2, annot=True, ax=axs[1], cmap='Blues', fmt=\".2f\", cbar=False)\n",
        "    axs[1].set_xlabel('Predicted labels')\n",
        "    axs[1].set_ylabel('True labels')\n",
        "    axs[1].set_title('Test Confusion Matrix')\n",
        "    axs[1].xaxis.set_ticklabels(labels)\n",
        "    axs[1].yaxis.set_ticklabels(labels)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "def evaluate_and_save_confusion_matrices(model, dataloader_test, dataloader_val, case_label):\n",
        "    # Compute confusion matrices\n",
        "    conf_matrix_val = compute_confusion_matrix(model, dataloader_val)\n",
        "    conf_matrix_test = compute_confusion_matrix(model, dataloader_test)\n",
        "\n",
        "    # Plot and save to single image\n",
        "    plot_two_confusion_matrices(conf_matrix_val, conf_matrix_test, range(10), f'combined_matrix_{case_label}.png')\n",
        "\n",
        "\n",
        "\n",
        "evaluate_and_save_confusion_matrices(model_a, dataloader_test, dataloader_val, 'a')\n",
        "evaluate_and_save_confusion_matrices(model_b, dataloader_test, dataloader_val, 'b')\n",
        "evaluate_and_save_confusion_matrices(model_c, dataloader_test, dataloader_val, 'c')\n",
        "evaluate_and_save_confusion_matrices(model_d, dataloader_test, dataloader_val, 'd')\n",
        "evaluate_and_save_confusion_matrices(model_e, dataloader_test, dataloader_val, 'e')\n",
        "evaluate_and_save_confusion_matrices(model_f, dataloader_test, dataloader_val, 'f')\n"
      ],
      "metadata": {
        "id": "iGg_G2JWm8mW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}